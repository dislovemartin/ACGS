"""
policy_synthesizer.py

This module defines the PolicySynthesizer, responsible for generating,
suggesting, and refining policies (both constitutional principles and
operational rules) using various techniques, including LLMs and potentially
formal synthesis methods.

Classes:
    PolicySynthesizer: Orchestrates policy synthesis.
    LLMPolicyGenerator: Uses an LLM to generate policy suggestions.
    # Future: FormalPolicySynthesizer, RuleBasedRefiner, etc.
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Union, Tuple

from alphaevolve_gs_engine.utils.logging_utils import setup_logger
from alphaevolve_gs_engine.core.constitutional_principle import ConstitutionalPrinciple
from alphaevolve_gs_engine.core.operational_rule import OperationalRule
from alphaevolve_gs_engine.services.llm_service import LLMService, get_llm_service
# from alphaevolve_gs_engine.services.validation.syntactic_validator import SyntacticValidator
# from alphaevolve_gs_engine.services.validation.semantic_validator import SemanticValidator, SemanticTestCase
# from alphaevolve_gs_engine.services.validation.safety_validator import SafetyValidator, SafetyAssertion
# Other validators can be added for iterative refinement.

logger = setup_logger(__name__)

class PolicySynthesisInput:
    """
    Represents the input for a policy synthesis task.

    Attributes:
        synthesis_goal (str): A natural language description of the desired policy's objective.
        policy_type (str): "constitutional_principle" or "operational_rule".
        existing_policies (Optional[List[Union[ConstitutionalPrinciple, OperationalRule]]]):
            Relevant existing policies that the new policy should be consistent with or build upon.
        constraints (Optional[List[str]]): Specific constraints or requirements for the new policy.
        context_data (Optional[Dict[str, Any]]): Additional contextual information.
        desired_format (str): "rego", "natural_language_structured", etc.
        target_id (Optional[str]): If refining an existing policy, its ID.
    """
    def __init__(self,
                 synthesis_goal: str,
                 policy_type: str, # "constitutional_principle" or "operational_rule"
                 desired_format: str = "rego",
                 existing_policies: Optional[List[Union[ConstitutionalPrinciple, OperationalRule]]] = None,
                 constraints: Optional[List[str]] = None,
                 context_data: Optional[Dict[str, Any]] = None,
                 target_id: Optional[str] = None): # For refinement
        self.synthesis_goal = synthesis_goal
        if policy_type not in ["constitutional_principle", "operational_rule"]:
            raise ValueError("policy_type must be 'constitutional_principle' or 'operational_rule'.")
        self.policy_type = policy_type
        self.desired_format = desired_format
        self.existing_policies = existing_policies if existing_policies else []
        self.constraints = constraints if constraints else []
        self.context_data = context_data if context_data else {}
        self.target_id = target_id

    def __repr__(self) -> str:
        return (f"PolicySynthesisInput(goal='{self.synthesis_goal[:50]}...', "
                f"type='{self.policy_type}', format='{self.desired_format}')")


class PolicySuggestion:
    """
    Represents a policy suggestion generated by a synthesizer.

    Attributes:
        suggested_policy_code (str): The generated policy code (e.g., Rego).
        explanation (str): An explanation of how the policy meets the goal.
        confidence_score (Optional[float]): Synthesizer's confidence in the suggestion.
        source_synthesizer (str): Name of the synthesizer that generated this.
        metadata (Optional[Dict[str, Any]]): Additional metadata.
    """
    def __init__(self,
                 suggested_policy_code: str,
                 explanation: str,
                 source_synthesizer: str,
                 confidence_score: Optional[float] = None,
                 metadata: Optional[Dict[str, Any]] = None):
        self.suggested_policy_code = suggested_policy_code
        self.explanation = explanation
        self.source_synthesizer = source_synthesizer
        self.confidence_score = confidence_score
        self.metadata = metadata if metadata else {}

    def __repr__(self) -> str:
        return (f"PolicySuggestion(source='{self.source_synthesizer}', "
                f"confidence={self.confidence_score or 'N/A'}, "
                f"code_snippet='{self.suggested_policy_code[:100].strip()}...')")


class PolicySynthesizer(ABC):
    """
    Abstract base class for policy synthesizers.
    """

    @abstractmethod
    def synthesize_policy(self,
                          s_input: PolicySynthesisInput
                         ) -> Optional[PolicySuggestion]:
        """
        Generates or refines a policy based on the synthesis input.

        Args:
            s_input (PolicySynthesisInput): The requirements for the policy.

        Returns:
            Optional[PolicySuggestion]: The suggested policy, or None if synthesis fails.
        """
        pass


class LLMPolicyGenerator(PolicySynthesizer):
    """
    Uses a Large Language Model (LLM) to generate policy suggestions.
    """
    def __init__(self, llm_service: LLMService):
        self.llm_service = llm_service
        logger.info(f"LLMPolicyGenerator initialized with LLM service: {type(llm_service).__name__}")

    def _construct_prompt(self, s_input: PolicySynthesisInput) -> str:
        """Constructs a detailed prompt for the LLM."""
        prompt = f"You are an expert AI policy engineer. Your task is to generate a new {s_input.policy_type}."
        if s_input.target_id:
            prompt += f" This is a refinement of an existing policy with ID '{s_input.target_id}'."
        prompt += f"\nThe primary goal for this policy is: {s_input.synthesis_goal}\n"

        prompt += f"The desired output format for the policy code is: {s_input.desired_format}.\n"
        if s_input.desired_format == "rego":
            prompt += ("Ensure the Rego code is syntactically correct, follows best practices, "
                       "and includes a clear package declaration (e.g., `package system.generated_policies`).\n")

        if s_input.existing_policies:
            prompt += "\nConsider the following existing policies for context and consistency:\n"
            for i, policy in enumerate(s_input.existing_policies):
                policy_id = getattr(policy, 'principle_id', getattr(policy, 'rule_id', f"UnknownPolicy_{i}"))
                policy_desc = getattr(policy, 'description', 'N/A')
                prompt += f"- Policy ID {policy_id}: {policy_desc}\n" # Could include code snippets too

        if s_input.constraints:
            prompt += "\nThe new policy MUST adhere to the following constraints:\n"
            for constraint in s_input.constraints:
                prompt += f"- {constraint}\n"

        if s_input.context_data:
            prompt += "\nAdditional context to consider:\n"
            for key, value in s_input.context_data.items():
                prompt += f"- {key}: {str(value)[:200]}\n" # Truncate long context values

        prompt += "\nPlease provide the generated policy code and a brief explanation of how it achieves the goal."
        prompt += "\nStructure your response as follows:\n"
        prompt += "POLICY_CODE_START\n[Your generated policy code here]\nPOLICY_CODE_END\n"
        prompt += "EXPLANATION_START\n[Your explanation here]\nEXPLANATION_END\n"
        
        # Optional: Add a request for confidence score if the LLM supports it or can be prompted for it.
        # prompt += "CONFIDENCE_SCORE_START\n[A score from 0.0 to 1.0 indicating your confidence]\nCONFIDENCE_SCORE_END\n"

        return prompt

    def _parse_llm_response(self, llm_output: str) -> Tuple[Optional[str], Optional[str], Optional[float]]:
        """Parses the LLM's response to extract code, explanation, and confidence."""
        policy_code = None
        explanation = None
        confidence = None # Not reliably extracted without specific LLM features

        try:
            code_match = re.search(r"POLICY_CODE_START\n(.*?)POLICY_CODE_END", llm_output, re.DOTALL)
            if code_match:
                policy_code = code_match.group(1).strip()
            
            explanation_match = re.search(r"EXPLANATION_START\n(.*?)EXPLANATION_END", llm_output, re.DOTALL)
            if explanation_match:
                explanation = explanation_match.group(1).strip()
            
            # Example for confidence (if LLM was prompted and provided it)
            # confidence_match = re.search(r"CONFIDENCE_SCORE_START\n(.*?)\nCONFIDENCE_SCORE_END", llm_output, re.DOTALL)
            # if confidence_match:
            #     try:
            #         confidence = float(confidence_match.group(1).strip())
            #     except ValueError:
            #         logger.warning(f"Could not parse confidence score from LLM output: {confidence_match.group(1).strip()}")

        except Exception as e:
            logger.error(f"Error parsing LLM response: {e}. Raw output: {llm_output[:300]}", exc_info=True)
            return None, None, None # Return None for all if parsing fails badly
        
        if not policy_code and not explanation: # If nothing was parsed
            logger.warning(f"Could not parse policy code or explanation from LLM response. Using full output as explanation. Output: {llm_output[:300]}")
            # Fallback: Use the whole output as explanation if parsing fails completely, no code.
            return None, llm_output, None


        return policy_code, explanation, confidence


    def synthesize_policy(self,
                          s_input: PolicySynthesisInput
                         ) -> Optional[PolicySuggestion]:
        prompt = self._construct_prompt(s_input)
        logger.debug(f"LLM Prompt for policy synthesis ({s_input.policy_type}, goal: '{s_input.synthesis_goal}'):\n{prompt}")

        try:
            # Using generate_text for now. If LLM supports structured output (JSON mode),
            # that would be more robust for parsing policy_code and explanation.
            # This might require a model that supports JSON mode and careful prompting.
            # For instance, asking the LLM to return a JSON object:
            # {"policy_code": "...", "explanation": "...", "confidence_score": 0.X}
            # llm_response_raw = self.llm_service.generate_structured_output(
            #     prompt, 
            #     output_format={"policy_code": "string", "explanation": "string", "confidence_score": "float"}
            # )
            # policy_code = llm_response_raw.get("policy_code")
            # explanation = llm_response_raw.get("explanation")
            # confidence = llm_response_raw.get("confidence_score")
            
            # Using simple text generation and parsing for broader compatibility:
            llm_raw_text_response = self.llm_service.generate_text(prompt, max_tokens=2048, temperature=0.5) # Adjust tokens/temp as needed
            
            policy_code, explanation, confidence = self._parse_llm_response(llm_raw_text_response)

            if not policy_code:
                logger.error(f"LLM failed to generate policy code for goal: {s_input.synthesis_goal}. Raw response: {llm_raw_text_response[:500]}")
                # Fallback to using the raw response as explanation if code is missing
                if not explanation:
                    explanation = f"LLM response did not yield parseable code: {llm_raw_text_response[:500]}"
                return PolicySuggestion(
                    suggested_policy_code="", # Empty code
                    explanation=explanation,
                    source_synthesizer=type(self).__name__,
                    confidence_score=0.0 # Low confidence if no code
                )
            
            if not explanation: # If code is there but no explanation
                explanation = "LLM generated policy code but explanation was not parsable or provided."
                logger.warning(explanation + f" Raw response: {llm_raw_text_response[:500]}")


            suggestion = PolicySuggestion(
                suggested_policy_code=policy_code,
                explanation=explanation,
                confidence_score=confidence, # May be None
                source_synthesizer=type(self).__name__
            )
            logger.info(f"LLM generated policy suggestion for goal: '{s_input.synthesis_goal}'. "
                        f"Code snippet: {policy_code[:100].strip()}...")
            return suggestion

        except Exception as e:
            logger.error(f"Error during LLM policy synthesis for goal '{s_input.synthesis_goal}': {e}", exc_info=True)
            return None


# Example Usage:
if __name__ == "__main__":
    import re # For parsing in the main example, if not using the class's internal parsing

    # Use MockLLMService for example to avoid actual API calls
    mock_llm = get_llm_service("mock")
    
    # Initialize the synthesizer
    llm_synthesizer = LLMPolicyGenerator(llm_service=mock_llm)

    # --- Example 1: Synthesize a new Operational Rule in Rego ---
    print("\n--- Example 1: New Operational Rule (Rego) ---")
    op_rule_input = PolicySynthesisInput(
        synthesis_goal="Create an operational rule that denies access to users from 'external_domain.com' "
                       "if they try to access resources tagged as 'confidential'.",
        policy_type="operational_rule",
        desired_format="rego",
        constraints=["The rule must be in a package named 'company.access_control'.",
                     "The denial reason should be logged (conceptually)."],
        context_data={"relevant_tags": ["confidential", "internal", "public"]}
    )

    op_rule_suggestion = llm_synthesizer.synthesize_policy(op_rule_input)

    if op_rule_suggestion:
        print(f"Suggestion Source: {op_rule_suggestion.source_synthesizer}")
        print(f"Confidence: {op_rule_suggestion.confidence_score if op_rule_suggestion.confidence_score is not None else 'N/A'}")
        print("Suggested Policy Code (Rego):")
        print(op_rule_suggestion.suggested_policy_code)
        print("\nExplanation:")
        print(op_rule_suggestion.explanation)
        # Basic assertion for mock - mock LLM usually includes "Mock response" and "Rego code"
        assert "Mock response" in op_rule_suggestion.explanation or "mock policy" in op_rule_suggestion.suggested_policy_code.lower()
        assert "package company.access_control" in op_rule_suggestion.suggested_policy_code # Check if package constraint was met
    else:
        print("Policy synthesis failed for the operational rule.")

    # --- Example 2: Synthesize a Constitutional Principle (Natural Language) ---
    print("\n--- Example 2: New Constitutional Principle (Natural Language) ---")
    # For this, the MockLLMService might not give a great "natural language" policy.
    # A real LLM would be better. We'll adjust the prompt slightly for the mock.
    cp_input = PolicySynthesisInput(
        synthesis_goal="Draft a constitutional principle ensuring AI systems prioritize human well-being "
                       "and safety above operational efficiency.",
        policy_type="constitutional_principle",
        desired_format="structured_natural_language", # LLM should output text
        existing_policies=[], # No existing policies for this simple example
        constraints=["The principle should be concise and unambiguous."]
    )
    # Adjust prompt for mock LLM to guide it better for non-Rego
    original_construct_prompt = llm_synthesizer._construct_prompt 
    def mock_construct_prompt_for_nl(s_input: PolicySynthesisInput):
        base_prompt = original_construct_prompt(s_input)
        if s_input.desired_format == "structured_natural_language":
            return base_prompt.replace(
                "Ensure the Rego code is syntactically correct", 
                "Ensure the principle is clearly articulated in natural language"
            ).replace("package system.generated_policies", "a clear title or heading")
        return base_prompt
    
    llm_synthesizer._construct_prompt = mock_construct_prompt_for_nl # Temporarily override

    cp_suggestion = llm_synthesizer.synthesize_policy(cp_input)
    
    llm_synthesizer._construct_prompt = original_construct_prompt # Restore original

    if cp_suggestion:
        print(f"Suggestion Source: {cp_suggestion.source_synthesizer}")
        print("Suggested Principle Text:")
        print(cp_suggestion.suggested_policy_code) # This will be natural language text
        print("\nExplanation:")
        print(cp_suggestion.explanation)
        assert "Mock response" in cp_suggestion.explanation or "mock policy" in cp_suggestion.suggested_policy_code.lower()
        assert "human well-being" in cp_suggestion.suggested_policy_code.lower() or \
               "human well-being" in cp_suggestion.explanation.lower() # Check if goal was addressed
    else:
        print("Policy synthesis failed for the constitutional principle.")


    # --- Example 3: Refining an existing policy (conceptual) ---
    print("\n--- Example 3: Refine an Existing Policy (Conceptual) ---")
    refinement_input = PolicySynthesisInput(
        target_id="OPR001_Old",
        synthesis_goal="Modify the existing operational rule 'OPR001_Old' to also include a check "
                       "for 'high_sensitivity' resource tag, in addition to 'confidential'.",
        policy_type="operational_rule",
        desired_format="rego",
        # existing_policies could include the actual code of OPR001_Old if helpful
        constraints=["Maintain existing functionality for 'confidential' tag.",
                     "Update package name to 'company.access_control_v2'"]
    )
    
    ref_suggestion = llm_synthesizer.synthesize_policy(refinement_input)
    if ref_suggestion:
        print("Refined Policy Code Suggestion:")
        print(ref_suggestion.suggested_policy_code)
        print("\nExplanation:")
        print(ref_suggestion.explanation)
        assert "package company.access_control_v2" in ref_suggestion.suggested_policy_code
        assert "high_sensitivity" in ref_suggestion.suggested_policy_code or "high_sensitivity" in ref_suggestion.explanation
    else:
        print("Policy refinement failed.")

    print("\nPolicy synthesizer examples completed.")
