#!/usr/bin/env python3
"""
Technical Review Validation Script

This script addresses all the critical issues identified in the technical review
of the AlphaEvolve-ACGS paper and validates the corrective actions.
"""

import asyncio
import sys
import os
import json
import logging
from pathlib import Path

# Add backend paths to Python path
sys.path.append(str(Path(__file__).parent.parent / "backend" / "gs_service"))
sys.path.append(str(Path(__file__).parent.parent / "backend" / "fv_service"))
sys.path.append(str(Path(__file__).parent.parent / "backend" / "integrity_service"))

from app.services.lipschitz_estimator import LipschitzEstimator, MetricSpaceValidator
from app.services.fairness_evaluation_framework import FairnessEvaluationFramework, DomainType
from app.core.verification_completeness_tester import VerificationCompletenessTester
from app.core.crypto_benchmarking import CryptoBenchmarker, CryptoBenchmarkConfig

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TechnicalReviewValidator:
    """Validates fixes for technical review findings."""
    
    def __init__(self):
        self.results = {}
        self.issues_found = []
        self.fixes_validated = []
    
    async def run_comprehensive_validation(self) -> dict:
        """Run all validation tests for technical review findings."""
        logger.info("Starting comprehensive technical review validation")
        
        # Issue 1: Inconsistent contraction bound
        await self._validate_lipschitz_bounds()
        
        # Issue 2: Metric space assumptions
        await self._validate_metric_properties()
        
        # Issue 3: Fairness claims in inappropriate domains
        await self._validate_fairness_evaluation()
        
        # Issue 4: Formal verification completeness
        await self._validate_verification_completeness()
        
        # Issue 5: Cryptographic overhead measurements
        await self._validate_crypto_overhead()
        
        # Generate final report
        return self._generate_validation_report()
    
    async def _validate_lipschitz_bounds(self):
        """Validate Lipschitz constant estimation and bounds."""
        logger.info("Validating Lipschitz constant estimation...")
        
        try:
            estimator = LipschitzEstimator()
            await estimator.initialize()
            
            # Test with sample principles
            test_principles = [
                "AI systems must not cause harm to humans",
                "AI decisions must be explainable and transparent",
                "AI systems must respect user privacy",
                "AI systems must be fair and unbiased"
            ]
            
            # Estimate LLM Lipschitz constant
            llm_result = await estimator.estimate_llm_lipschitz_constant(test_principles)
            
            self.results["lipschitz_validation"] = {
                "estimated_constant": llm_result.estimated_constant,
                "confidence_interval": llm_result.confidence_interval,
                "num_samples": llm_result.num_samples,
                "methodology": llm_result.methodology,
                "status": "FIXED" if llm_result.num_samples > 0 else "FAILED"
            }
            
            if llm_result.num_samples > 0:
                self.fixes_validated.append("Lipschitz constant estimation with confidence intervals")
                
                # Check if empirical bound is consistent with theory
                if llm_result.estimated_constant > 1.0:
                    self.issues_found.append(
                        f"Estimated Lipschitz constant {llm_result.estimated_constant:.3f} > 1.0 - "
                        "contraction property not satisfied"
                    )
                else:
                    logger.info(f"‚úÖ Lipschitz constant {llm_result.estimated_constant:.3f} < 1.0 - contraction satisfied")
            else:
                self.issues_found.append("Lipschitz constant estimation failed - no samples collected")
                
        except Exception as e:
            logger.error(f"Lipschitz validation failed: {e}")
            self.results["lipschitz_validation"] = {"status": "ERROR", "error": str(e)}
            self.issues_found.append(f"Lipschitz validation error: {e}")
    
    async def _validate_metric_properties(self):
        """Validate metric space properties of distance function."""
        logger.info("Validating metric space properties...")
        
        try:
            estimator = LipschitzEstimator()
            
            test_principles = [
                "AI systems must not cause harm",
                "AI decisions must be transparent",
                "AI systems must respect privacy",
                "AI systems must be fair"
            ]
            
            metric_validation = await estimator.validate_metric_properties(test_principles)
            
            self.results["metric_validation"] = metric_validation
            
            if metric_validation["is_valid_metric"]:
                self.fixes_validated.append("Distance function satisfies metric properties")
                logger.info("‚úÖ Distance function is a valid metric")
            else:
                triangle_violations = metric_validation["triangle_inequality"]["violation_rate"]
                symmetry_violations = metric_validation["symmetry"]["violation_rate"]
                
                if triangle_violations > 0:
                    self.issues_found.append(
                        f"Triangle inequality violated in {triangle_violations:.1%} of tests"
                    )
                
                if symmetry_violations > 0:
                    self.issues_found.append(
                        f"Symmetry violated in {symmetry_violations:.1%} of tests"
                    )
                    
        except Exception as e:
            logger.error(f"Metric validation failed: {e}")
            self.results["metric_validation"] = {"status": "ERROR", "error": str(e)}
            self.issues_found.append(f"Metric validation error: {e}")
    
    async def _validate_fairness_evaluation(self):
        """Validate fairness evaluation framework."""
        logger.info("Validating fairness evaluation framework...")
        
        try:
            framework = FairnessEvaluationFramework()
            
            # Test arithmetic domain (should not evaluate fairness)
            arithmetic_result = framework.evaluate_domain_fairness(
                domain_type=DomainType.ARITHMETIC,
                predictions=[1, 0, 1, 0],
                ground_truth=[1, 0, 1, 1],
                protected_attributes={}
            )
            
            # Test hiring domain (should evaluate fairness)
            hiring_result = framework.evaluate_domain_fairness(
                domain_type=DomainType.HIRING,
                predictions=[1, 0, 1, 0, 1, 0, 1, 0],
                ground_truth=[1, 0, 1, 1, 1, 0, 0, 0],
                protected_attributes={
                    "gender": ["male", "female", "male", "female", "male", "female", "male", "female"],
                    "race": ["white", "black", "white", "black", "white", "black", "white", "black"]
                }
            )
            
            self.results["fairness_validation"] = {
                "arithmetic_domain": arithmetic_result,
                "hiring_domain": hiring_result
            }
            
            # Validate that arithmetic domain correctly rejects fairness evaluation
            if not arithmetic_result["fairness_applicable"]:
                self.fixes_validated.append("Arithmetic domain correctly excludes fairness evaluation")
                logger.info("‚úÖ Arithmetic domain correctly excludes fairness evaluation")
            else:
                self.issues_found.append("Arithmetic domain incorrectly includes fairness evaluation")
            
            # Validate that hiring domain correctly applies fairness evaluation
            if hiring_result["fairness_applicable"]:
                self.fixes_validated.append("Hiring domain correctly includes fairness evaluation")
                logger.info("‚úÖ Hiring domain correctly includes fairness evaluation")
            else:
                self.issues_found.append("Hiring domain incorrectly excludes fairness evaluation")
                
        except Exception as e:
            logger.error(f"Fairness validation failed: {e}")
            self.results["fairness_validation"] = {"status": "ERROR", "error": str(e)}
            self.issues_found.append(f"Fairness validation error: {e}")
    
    async def _validate_verification_completeness(self):
        """Validate formal verification completeness."""
        logger.info("Validating formal verification completeness...")
        
        try:
            tester = VerificationCompletenessTester()
            completeness_results = await tester.run_completeness_tests()
            
            self.results["verification_completeness"] = completeness_results
            
            # Check completeness score
            completeness_score = completeness_results["verification_completeness_score"]
            if completeness_score >= 0.7:
                self.fixes_validated.append(f"Verification completeness score: {completeness_score:.2f}")
                logger.info(f"‚úÖ Verification completeness score acceptable: {completeness_score:.2f}")
            else:
                self.issues_found.append(f"Low verification completeness score: {completeness_score:.2f}")
            
            # Check positive/negative case differentiation
            positive_rate = completeness_results["positive_case_pass_rate"]
            negative_rate = completeness_results["negative_case_pass_rate"]
            
            if positive_rate >= 0.8 and negative_rate >= 0.8:
                self.fixes_validated.append("Verification properly differentiates positive/negative cases")
                logger.info("‚úÖ Verification properly differentiates positive/negative cases")
            else:
                if positive_rate < 0.8:
                    self.issues_found.append(f"Low positive case pass rate: {positive_rate:.2f}")
                if negative_rate < 0.8:
                    self.issues_found.append(f"Low negative case pass rate: {negative_rate:.2f}")
                    
        except Exception as e:
            logger.error(f"Verification completeness validation failed: {e}")
            self.results["verification_completeness"] = {"status": "ERROR", "error": str(e)}
            self.issues_found.append(f"Verification completeness error: {e}")
    
    async def _validate_crypto_overhead(self):
        """Validate cryptographic overhead measurements."""
        logger.info("Validating cryptographic overhead measurements...")
        
        try:
            config = CryptoBenchmarkConfig(
                num_iterations=20,  # Reduced for validation
                payload_sizes=[1024, 4096],
                key_sizes=[2048]
            )
            
            benchmarker = CryptoBenchmarker(config)
            crypto_results = await benchmarker.benchmark_all_operations()
            
            self.results["crypto_overhead"] = crypto_results
            
            # Validate that measurements include confidence intervals
            overhead_analysis = crypto_results["system_overhead_analysis"]
            
            if "throughput_impact_percent" in overhead_analysis:
                self.fixes_validated.append("Cryptographic overhead properly measured with system impact")
                logger.info("‚úÖ Cryptographic overhead properly measured")
                
                # Check for reasonable overhead values
                throughput_impact = overhead_analysis["throughput_impact_percent"]
                latency_overhead = overhead_analysis["latency_overhead_ms"]
                
                if throughput_impact > 50:
                    self.issues_found.append(f"Very high throughput impact: {throughput_impact:.1f}%")
                
                if latency_overhead > 100:
                    self.issues_found.append(f"Very high latency overhead: {latency_overhead:.1f}ms")
                    
            else:
                self.issues_found.append("Cryptographic overhead analysis incomplete")
                
        except Exception as e:
            logger.error(f"Crypto overhead validation failed: {e}")
            self.results["crypto_overhead"] = {"status": "ERROR", "error": str(e)}
            self.issues_found.append(f"Crypto overhead validation error: {e}")
    
    def _generate_validation_report(self) -> dict:
        """Generate comprehensive validation report."""
        total_fixes = len(self.fixes_validated)
        total_issues = len(self.issues_found)
        
        # Calculate overall validation score
        validation_score = total_fixes / (total_fixes + total_issues) if (total_fixes + total_issues) > 0 else 0
        
        report = {
            "validation_summary": {
                "total_fixes_validated": total_fixes,
                "total_issues_remaining": total_issues,
                "validation_score": validation_score,
                "overall_status": "PASSED" if validation_score >= 0.8 else "NEEDS_IMPROVEMENT"
            },
            "fixes_validated": self.fixes_validated,
            "issues_remaining": self.issues_found,
            "detailed_results": self.results,
            "recommendations": self._generate_recommendations()
        }
        
        return report
    
    def _generate_recommendations(self) -> list:
        """Generate recommendations based on validation results."""
        recommendations = []
        
        if any("Lipschitz" in issue for issue in self.issues_found):
            recommendations.append(
                "Re-estimate Lipschitz constants with larger sample sizes and document methodology"
            )
        
        if any("metric" in issue.lower() for issue in self.issues_found):
            recommendations.append(
                "Replace cosine similarity with proper metric distance function"
            )
        
        if any("verification" in issue.lower() for issue in self.issues_found):
            recommendations.append(
                "Improve SMT encoding to better differentiate positive/negative cases"
            )
        
        if any("crypto" in issue.lower() for issue in self.issues_found):
            recommendations.append(
                "Optimize cryptographic operations or implement async processing"
            )
        
        if not self.issues_found:
            recommendations.append(
                "All technical review issues have been addressed. Continue monitoring in production."
            )
        
        return recommendations


async def main():
    """Main validation function."""
    print("üîç Starting Technical Review Validation")
    print("=" * 60)
    
    validator = TechnicalReviewValidator()
    report = await validator.run_comprehensive_validation()
    
    # Print summary
    print("\nüìä VALIDATION SUMMARY")
    print("=" * 60)
    print(f"Fixes Validated: {report['validation_summary']['total_fixes_validated']}")
    print(f"Issues Remaining: {report['validation_summary']['total_issues_remaining']}")
    print(f"Validation Score: {report['validation_summary']['validation_score']:.2f}")
    print(f"Overall Status: {report['validation_summary']['overall_status']}")
    
    # Print fixes validated
    if report['fixes_validated']:
        print("\n‚úÖ FIXES VALIDATED:")
        for fix in report['fixes_validated']:
            print(f"  ‚Ä¢ {fix}")
    
    # Print remaining issues
    if report['issues_remaining']:
        print("\n‚ö†Ô∏è  ISSUES REMAINING:")
        for issue in report['issues_remaining']:
            print(f"  ‚Ä¢ {issue}")
    
    # Print recommendations
    if report['recommendations']:
        print("\nüí° RECOMMENDATIONS:")
        for rec in report['recommendations']:
            print(f"  ‚Ä¢ {rec}")
    
    # Save detailed report
    report_path = Path(__file__).parent.parent / "technical_review_validation_report.json"
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\nüìÑ Detailed report saved to: {report_path}")
    
    # Exit with appropriate code
    if report['validation_summary']['overall_status'] == "PASSED":
        print("\nüéâ Technical review validation PASSED!")
        sys.exit(0)
    else:
        print("\n‚ùå Technical review validation needs improvement.")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
