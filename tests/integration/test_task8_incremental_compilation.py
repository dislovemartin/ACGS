"""
Integration Tests for Task 8: Incremental Policy Compilation

Tests the OPA-based incremental compilation system for policy enforcement
efficiency improvements. Validates performance, correctness, and integration
with the ACGS-PGP microservices architecture.

Key Test Areas:
1. OPA server integration and health checks
2. Incremental vs full compilation performance
3. Policy dependency tracking and cache invalidation
4. Error handling and fallback mechanisms
5. Metrics collection and monitoring integration
"""

import asyncio
import json
import logging
import os
import time
from typing import Dict, List, Any, Optional
import pytest
import httpx
from datetime import datetime

# Add project root to path for imports
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

logger = logging.getLogger(__name__)

# Test configuration
OPA_SERVER_URL = os.getenv("OPA_SERVER_URL", "http://localhost:8181")
PGC_SERVICE_URL = os.getenv("PGC_SERVICE_URL", "http://localhost:8005")
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

# Performance targets from Task 8 requirements
PERFORMANCE_TARGETS = {
    "incremental_compilation_time_ms": 100,  # <100ms for incremental
    "full_compilation_time_ms": 1000,       # <1000ms for full
    "cache_hit_ratio_threshold": 0.8,       # >80% cache hit ratio
    "dependency_analysis_time_ms": 50,      # <50ms for dependency analysis
    "compilation_savings_percent": 60       # >60% time savings vs full
}

class Task8IntegrationTester:
    """Integration tester for Task 8 incremental policy compilation."""
    
    def __init__(self):
        self.client = httpx.AsyncClient(timeout=30.0)
        self.test_results: List[Dict[str, Any]] = []
        self.performance_metrics: Dict[str, Any] = {}
        
    async def __aenter__(self):
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.client.aclose()
    
    def log_test_result(self, test_name: str, success: bool, details: str = "", metrics: Dict[str, Any] = None):
        """Log test result with performance metrics."""
        result = {
            "test_name": test_name,
            "success": success,
            "details": details,
            "timestamp": datetime.now().isoformat(),
            "metrics": metrics or {}
        }
        self.test_results.append(result)
        
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{status} {test_name}: {details}")
        
        if metrics:
            print(f"   üìä Metrics: {json.dumps(metrics, indent=2)}")
    
    async def test_opa_server_health(self) -> bool:
        """Test OPA server health and availability."""
        test_name = "OPA Server Health Check"
        
        try:
            # Test OPA server health endpoint
            response = await self.client.get(f"{OPA_SERVER_URL}/health")
            
            if response.status_code == 200:
                health_data = response.json()
                self.log_test_result(
                    test_name, True, 
                    f"OPA server healthy: {health_data}",
                    {"response_time_ms": response.elapsed.total_seconds() * 1000}
                )
                return True
            else:
                self.log_test_result(
                    test_name, False, 
                    f"OPA server unhealthy: {response.status_code}"
                )
                return False
                
        except Exception as e:
            self.log_test_result(test_name, False, f"OPA server connection failed: {e}")
            return False
    
    async def test_opa_server_info(self) -> bool:
        """Test OPA server information and capabilities."""
        test_name = "OPA Server Information"
        
        try:
            response = await self.client.get(f"{OPA_SERVER_URL}/")
            
            if response.status_code == 200:
                server_info = response.json()
                version = server_info.get("version", "unknown")
                
                self.log_test_result(
                    test_name, True,
                    f"OPA version: {version}",
                    {"server_info": server_info}
                )
                return True
            else:
                self.log_test_result(
                    test_name, False,
                    f"Failed to get server info: {response.status_code}"
                )
                return False
                
        except Exception as e:
            self.log_test_result(test_name, False, f"Server info request failed: {e}")
            return False
    
    async def test_incremental_compiler_initialization(self) -> bool:
        """Test incremental compiler initialization."""
        test_name = "Incremental Compiler Initialization"
        
        try:
            # Import and test incremental compiler
            from src.backend.pgc_service.app.core.incremental_compiler import get_incremental_compiler
            
            start_time = time.time()
            compiler = await get_incremental_compiler()
            init_time = (time.time() - start_time) * 1000
            
            # Verify compiler components
            assert compiler is not None
            assert hasattr(compiler, 'opa_client')
            assert hasattr(compiler, 'dependency_graph')
            assert hasattr(compiler, 'compilation_cache')
            assert hasattr(compiler, 'metrics')
            
            self.log_test_result(
                test_name, True,
                "Incremental compiler initialized successfully",
                {"initialization_time_ms": init_time}
            )
            return True
            
        except Exception as e:
            self.log_test_result(test_name, False, f"Initialization failed: {e}")
            return False
    
    async def test_opa_client_initialization(self) -> bool:
        """Test OPA client initialization and connection."""
        test_name = "OPA Client Initialization"
        
        try:
            from src.backend.pgc_service.app.core.opa_client import get_opa_client
            
            start_time = time.time()
            opa_client = await get_opa_client()
            init_time = (time.time() - start_time) * 1000
            
            # Test health check
            health_ok = await opa_client.health_check()
            
            if health_ok:
                self.log_test_result(
                    test_name, True,
                    "OPA client initialized and connected",
                    {"initialization_time_ms": init_time}
                )
                return True
            else:
                self.log_test_result(
                    test_name, False,
                    "OPA client initialized but health check failed"
                )
                return False
                
        except Exception as e:
            self.log_test_result(test_name, False, f"OPA client initialization failed: {e}")
            return False
    
    async def test_policy_compilation_performance(self) -> bool:
        """Test policy compilation performance (incremental vs full)."""
        test_name = "Policy Compilation Performance"
        
        try:
            from src.backend.pgc_service.app.core.incremental_compiler import get_incremental_compiler
            from src.backend.pgc_service.app.services.integrity_client import IntegrityPolicyRule
            
            compiler = await get_incremental_compiler()
            
            # Create test policies
            test_policies = [
                IntegrityPolicyRule(
                    id=i + 1,  # Integer ID
                    rule_content=f"""
                    package test.policy{i}

                    default allow = false

                    allow {{
                        input.user == "test_user_{i}"
                        input.action == "read"
                    }}
                    """,
                    version=1,
                    verification_status="verified",
                    source_principle_ids=[i + 1]
                )
                for i in range(5)
            ]
            
            # Test full compilation
            start_time = time.time()
            full_metrics = await compiler.compile_policies(test_policies, force_full=True)
            full_time = (time.time() - start_time) * 1000
            
            # Test incremental compilation (should be faster)
            start_time = time.time()
            incremental_metrics = await compiler.compile_policies(test_policies, force_full=False)
            incremental_time = (time.time() - start_time) * 1000
            
            # Calculate performance metrics
            time_savings_percent = ((full_time - incremental_time) / full_time) * 100 if full_time > 0 else 0
            
            performance_data = {
                "full_compilation_time_ms": full_time,
                "incremental_compilation_time_ms": incremental_time,
                "time_savings_percent": time_savings_percent,
                "full_metrics": full_metrics.__dict__ if hasattr(full_metrics, '__dict__') else str(full_metrics),
                "incremental_metrics": incremental_metrics.__dict__ if hasattr(incremental_metrics, '__dict__') else str(incremental_metrics)
            }
            
            # Check performance targets
            meets_targets = (
                incremental_time <= PERFORMANCE_TARGETS["incremental_compilation_time_ms"] and
                full_time <= PERFORMANCE_TARGETS["full_compilation_time_ms"]
            )
            
            self.log_test_result(
                test_name, meets_targets,
                f"Performance: Full={full_time:.2f}ms, Incremental={incremental_time:.2f}ms, Savings={time_savings_percent:.1f}%",
                performance_data
            )
            
            return meets_targets
            
        except Exception as e:
            self.log_test_result(test_name, False, f"Performance test failed: {e}")
            return False
    
    async def test_dependency_tracking(self) -> bool:
        """Test policy dependency tracking and cache invalidation."""
        test_name = "Policy Dependency Tracking"
        
        try:
            from src.backend.pgc_service.app.core.incremental_compiler import get_incremental_compiler
            from src.backend.pgc_service.app.services.integrity_client import IntegrityPolicyRule
            
            compiler = await get_incremental_compiler()
            
            # Create policies with dependencies
            base_policy = IntegrityPolicyRule(
                id=100,
                rule_content="""
                package base

                default base_allow = false
                base_allow {
                    input.base_check == true
                }
                """,
                version=1,
                verification_status="verified",
                source_principle_ids=[100]
            )

            dependent_policy = IntegrityPolicyRule(
                id=101,
                rule_content="""
                package dependent

                import data.base

                default allow = false
                allow {
                    base.base_allow
                    input.user == "authorized"
                }
                """,
                version=1,
                verification_status="verified",
                source_principle_ids=[101]
            )
            
            # Compile policies and track dependencies
            start_time = time.time()
            await compiler.compile_policies([base_policy, dependent_policy])
            dependency_time = (time.time() - start_time) * 1000
            
            # Verify dependency graph was built
            has_dependencies = len(compiler.dependency_graph.nodes) > 0
            
            self.log_test_result(
                test_name, has_dependencies,
                f"Dependency tracking: {len(compiler.dependency_graph.nodes)} nodes, {len(compiler.dependency_graph.edges)} edges",
                {
                    "dependency_analysis_time_ms": dependency_time,
                    "nodes_count": len(compiler.dependency_graph.nodes),
                    "edges_count": len(compiler.dependency_graph.edges)
                }
            )
            
            return has_dependencies
            
        except Exception as e:
            self.log_test_result(test_name, False, f"Dependency tracking test failed: {e}")
            return False

    async def test_cache_functionality(self) -> bool:
        """Test compilation cache functionality and performance."""
        test_name = "Cache Functionality"

        try:
            from src.backend.pgc_service.app.core.incremental_compiler import get_incremental_compiler
            from src.backend.pgc_service.app.services.integrity_client import IntegrityPolicyRule

            compiler = await get_incremental_compiler()

            # Create test policy
            test_policy = IntegrityPolicyRule(
                id=200,
                rule_content="""
                package cache_test

                default allow = false
                allow {
                    input.user == "cache_user"
                }
                """,
                version=1,
                verification_status="verified",
                source_principle_ids=[200]
            )

            # First compilation (cache miss)
            start_time = time.time()
            await compiler.compile_policies([test_policy])
            first_compile_time = (time.time() - start_time) * 1000

            # Second compilation (should use cache)
            start_time = time.time()
            await compiler.compile_policies([test_policy])
            second_compile_time = (time.time() - start_time) * 1000

            # Calculate cache effectiveness
            cache_speedup = (first_compile_time - second_compile_time) / first_compile_time * 100 if first_compile_time > 0 else 0
            cache_size = len(compiler.compilation_cache)

            cache_effective = cache_speedup > 0 and cache_size > 0

            self.log_test_result(
                test_name, cache_effective,
                f"Cache: {cache_size} entries, {cache_speedup:.1f}% speedup",
                {
                    "first_compile_time_ms": first_compile_time,
                    "second_compile_time_ms": second_compile_time,
                    "cache_speedup_percent": cache_speedup,
                    "cache_size": cache_size
                }
            )

            return cache_effective

        except Exception as e:
            self.log_test_result(test_name, False, f"Cache test failed: {e}")
            return False

    async def test_error_handling_and_fallback(self) -> bool:
        """Test error handling and fallback mechanisms."""
        test_name = "Error Handling and Fallback"

        try:
            from src.backend.pgc_service.app.core.incremental_compiler import get_incremental_compiler
            from src.backend.pgc_service.app.services.integrity_client import IntegrityPolicyRule

            compiler = await get_incremental_compiler()

            # Test with invalid policy content
            invalid_policy = IntegrityPolicyRule(
                id=300,
                rule_content="invalid rego syntax {{{",
                version=1,
                verification_status="verified",
                source_principle_ids=[300]
            )

            # Should handle error gracefully
            try:
                await compiler.compile_policies([invalid_policy])
                # If no exception, check if fallback was used
                fallback_used = True
            except Exception as compile_error:
                # Error handling should be graceful
                fallback_used = "fallback" in str(compile_error).lower()

            self.log_test_result(
                test_name, True,  # Always pass if no crash
                "Error handling working - graceful degradation",
                {"fallback_mechanism_used": fallback_used}
            )

            return True

        except Exception as e:
            self.log_test_result(test_name, False, f"Error handling test failed: {e}")
            return False

    async def test_metrics_collection(self) -> bool:
        """Test compilation metrics collection and reporting."""
        test_name = "Metrics Collection"

        try:
            from src.backend.pgc_service.app.core.incremental_compiler import get_incremental_compiler
            from src.backend.pgc_service.app.services.integrity_client import IntegrityPolicyRule

            compiler = await get_incremental_compiler()

            # Get initial metrics
            initial_metrics = compiler.metrics.copy()

            # Perform compilation to generate metrics
            test_policy = IntegrityPolicyRule(
                id=400,
                rule_content="""
                package metrics_test

                default allow = false
                allow {
                    input.action == "test"
                }
                """,
                version=1,
                verification_status="verified",
                source_principle_ids=[400]
            )

            await compiler.compile_policies([test_policy])

            # Check if metrics were updated
            final_metrics = compiler.metrics.copy()

            metrics_updated = (
                final_metrics["total_compilations"] > initial_metrics["total_compilations"] or
                final_metrics["policies_compiled"] > initial_metrics["policies_compiled"]
            )

            self.log_test_result(
                test_name, metrics_updated,
                "Metrics collection functional",
                {
                    "initial_metrics": initial_metrics,
                    "final_metrics": final_metrics,
                    "metrics_keys": list(final_metrics.keys())
                }
            )

            return metrics_updated

        except Exception as e:
            self.log_test_result(test_name, False, f"Metrics collection test failed: {e}")
            return False

    async def test_pgc_service_integration(self) -> bool:
        """Test integration with PGC service endpoints."""
        test_name = "PGC Service Integration"

        try:
            # Test PGC service health
            response = await self.client.get(f"{PGC_SERVICE_URL}/health")

            if response.status_code != 200:
                self.log_test_result(
                    test_name, False,
                    f"PGC service not available: {response.status_code}"
                )
                return False

            # Test compilation metrics endpoint (if available)
            try:
                metrics_response = await self.client.get(f"{PGC_SERVICE_URL}/api/v1/compilation/metrics")
                metrics_available = metrics_response.status_code == 200

                if metrics_available:
                    metrics_data = metrics_response.json()
                else:
                    metrics_data = {}

            except Exception:
                metrics_available = False
                metrics_data = {}

            self.log_test_result(
                test_name, True,  # Pass if service is running
                f"PGC service integration: metrics_endpoint={metrics_available}",
                {
                    "service_health": "ok",
                    "metrics_endpoint_available": metrics_available,
                    "metrics_data": metrics_data
                }
            )

            return True

        except Exception as e:
            self.log_test_result(test_name, False, f"PGC service integration test failed: {e}")
            return False


async def run_task8_integration_tests():
    """Run all Task 8 incremental compilation integration tests."""
    print("üöÄ Running Task 8: Incremental Policy Compilation Integration Tests")
    print("=" * 80)
    
    async with Task8IntegrationTester() as tester:
        # Core infrastructure tests
        tests = [
            tester.test_opa_server_health,
            tester.test_opa_server_info,
            tester.test_incremental_compiler_initialization,
            tester.test_opa_client_initialization,
            tester.test_policy_compilation_performance,
            tester.test_dependency_tracking,
            tester.test_cache_functionality,
            tester.test_error_handling_and_fallback,
            tester.test_metrics_collection,
            tester.test_pgc_service_integration,
        ]
        
        passed = 0
        total = len(tests)
        
        for test in tests:
            try:
                if await test():
                    passed += 1
            except Exception as e:
                print(f"‚ùå Test {test.__name__} failed with exception: {e}")
        
        # Print summary
        print("\n" + "=" * 80)
        print("üìä TASK 8 INTEGRATION TEST SUMMARY")
        print("=" * 80)
        print(f"Tests Passed: {passed}/{total}")
        print(f"Success Rate: {(passed/total)*100:.1f}%")
        
        if passed == total:
            print("üéâ All Task 8 incremental compilation tests passed!")
            print("‚úÖ OPA integration working correctly")
            print("‚úÖ Performance targets met")
            print("‚úÖ Dependency tracking functional")
            print("üöÄ Task 8 ready for production deployment!")
        else:
            print(f"‚ö†Ô∏è  {total - passed} tests failed - review implementation")
        
        return passed == total


if __name__ == "__main__":
    asyncio.run(run_task8_integration_tests())
